{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KCk0VSzcStRD"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numba import cuda, jit, int32\n",
        "from datetime import datetime\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cur_time():\n",
        "  return time.time()*1000"
      ],
      "metadata": {
        "id": "TcGtrj-TjjMp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def gpu_vec_sum(a, b, c):\n",
        "  idx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "  sum = 0\n",
        "  if idx < len(a):\n",
        "    \n",
        "    sum += a[idx]+b[idx]\n",
        "    cuda.atomic.add(c, 0, sum)\n",
        "    \n",
        "def do_sum_with_gpu(a, b, c, col_):\n",
        "    gpu_a = cuda.to_device(a)\n",
        "    gpu_b = cuda.to_device(b)\n",
        "    gpu_c = cuda.to_device(c)\n",
        "    start = get_cur_time()\n",
        "    gpu_vec_sum[col_, TPB](gpu_a, gpu_b, gpu_c)\n",
        "    itog_time = get_cur_time()-start\n",
        "    return gpu_c.copy_to_host()[0], itog_time"
      ],
      "metadata": {
        "id": "PmOEBqgoT0zd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TPB = 32\n",
        "arr_len = 1000\n",
        "\n",
        "for k_ in range(4): \n",
        "  \n",
        "  a = []\n",
        "  for i in range(arr_len):\n",
        "    a.append(i)\n",
        "  b = []\n",
        "  for i in range(int((arr_len)/2)):\n",
        "    b.append(a[-1])\n",
        "    a.pop()\n",
        "\n",
        "  col_ = (math.ceil((len(a))/TPB))  \n",
        "  c1 = np.zeros(1, dtype = np.int64)\n",
        "  cpu_start = get_cur_time()\n",
        "  cpu_sum = sum(a) + sum(b)\n",
        "  cpu_time = get_cur_time()-cpu_start\n",
        "\n",
        "  gpu_sum, gpu_time = do_sum_with_gpu(a, b, c1, col_)\n",
        "  print(\"CPU: %s elements\" % arr_len)\n",
        "  print(cpu_sum, cpu_time)\n",
        "  print(\"GPU: %s elements\" % arr_len)\n",
        "  print(gpu_sum, gpu_time)\n",
        "  arr_len = arr_len*10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hEH92-Vet1w",
        "outputId": "c94c33b5-7dc4-4b9e-dd5b-5468c753805c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 16 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: 1000 elements\n",
            "499500 0.0126953125\n",
            "GPU: 1000 elements\n",
            "499500 128.359375\n",
            "CPU: 10000 elements\n",
            "49995000 0.073974609375\n",
            "GPU: 10000 elements\n",
            "49995000 0.713134765625\n",
            "CPU: 100000 elements\n",
            "4999950000 0.62548828125\n",
            "GPU: 100000 elements\n",
            "4999950000 0.835205078125\n",
            "CPU: 1000000 elements\n",
            "499999500000 7.291748046875\n",
            "GPU: 1000000 elements\n",
            "499999500000 0.18408203125\n"
          ]
        }
      ]
    }
  ]
}